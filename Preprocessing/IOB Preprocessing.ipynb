{"cells":[{"cell_type":"markdown","metadata":{"id":"UhIz2s0iCIEt"},"source":["# XML to Python"]},{"cell_type":"markdown","source":["17 Mar: Nichy stop at Step 4. Tim comes to help on IOB part.\n","\n","19 Mar: Nichy finished step 4 (converting to IOB format). Time is helping on extracting misspelling words."],"metadata":{"id":"6iXxWhnEJoxF"}},{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/gdrive')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"-M-ovCo5O-Pu","executionInfo":{"status":"ok","timestamp":1648877207389,"user_tz":-480,"elapsed":37000,"user":{"displayName":"Nichamon Han-idhikul","userId":"15474088881495434738"}},"outputId":"554aa92f-3fdc-457b-d446-dbe7bcbb4156"},"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/gdrive\n"]}]},{"cell_type":"code","source":["import nltk\n","nltk.download('punkt')\n","nltk.download('averaged_perceptron_tagger')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"6-0FEW_rChKY","executionInfo":{"status":"ok","timestamp":1648820305988,"user_tz":-480,"elapsed":2096,"user":{"displayName":"Nichamon Han-idhikul","userId":"15474088881495434738"}},"outputId":"3e063ad1-edb9-400c-9da7-4b7b9b440b90"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["[nltk_data] Downloading package punkt to /root/nltk_data...\n","[nltk_data]   Unzipping tokenizers/punkt.zip.\n","[nltk_data] Downloading package averaged_perceptron_tagger to\n","[nltk_data]     /root/nltk_data...\n","[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n"]},{"output_type":"execute_result","data":{"text/plain":["True"]},"metadata":{},"execution_count":3}]},{"cell_type":"markdown","metadata":{"id":"-n3JPdveCIE2"},"source":["## Get 'text', 'aspectTerm' and 'aspectCategory from XML\n","\n","The bold letters are what we get. I will use the same variable names as in XML, so we don't get confused.\n","\n","\n","\\<text\\>**But the staff was so horrible to us.**\\<\\/text\\>\n","\n","\\<aspectTerm **term=\"staff\" polarity=\"negative\"** from=\"8\" to=\"13\"\\/\\>\n","\n","\\<aspectCategory **category=\"service\" polarity=\"negative**\"\\/\\>\n"]},{"cell_type":"code","source":["!ls '/content/gdrive/MyDrive/Colab Notebooks/CS4248 NLP Project/'"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"XKii3pZlPWUt","executionInfo":{"status":"ok","timestamp":1648878135883,"user_tz":-480,"elapsed":637,"user":{"displayName":"Nichamon Han-idhikul","userId":"15474088881495434738"}},"outputId":"9b4a3b4b-6060-46eb-fbcb-0aa775b7be46"},"execution_count":27,"outputs":[{"output_type":"stream","name":"stdout","text":["'Instruction to use Colab.ipynb'\t pre-process.ipynb\n","'IOB Preprocessing.ipynb'\t\t Restaurants_Test.xml\n"," pre_processed.csv\t\t\t Restaurants_Train.xml\n","'Preprocessing xml to dataframe.ipynb'\t'Unsupervised Model.ipynb'\n"]}]},{"cell_type":"code","execution_count":2,"metadata":{"id":"rl23ldnXCIE4","executionInfo":{"status":"ok","timestamp":1648877209732,"user_tz":-480,"elapsed":1900,"user":{"displayName":"Nichamon Han-idhikul","userId":"15474088881495434738"}}},"outputs":[],"source":["import xml.etree.ElementTree as ET\n","\n","restaurant = ET.parse('/content/gdrive/MyDrive/Colab Notebooks/CS4248 NLP Project/Restaurants_Train.xml')\n","sentences = restaurant.getroot()\n","\n","# List: a list of 'text' or a sentence from XML.\n","texts = []\n","# List: a list of 'term' in each text\n","terms = []\n","# List: a list of 'category' in each text\n","categories = []\n","\n","# Dictionary: {term: number of terms founded in all texts}\n","term_positive = {}\n","term_negative = {}\n","term_neutral  = {}\n","term_conflict = {}\n","\n","# Dictionary: {category: number of category founded in all texts}\n","category_positive = {}\n","category_negative = {}\n","category_neutral  = {}\n","category_conflict = {}\n","\n","\n","for sentence in sentences:\n","    hasTerm, hasCategory = False, False\n","    for taa in sentence: # taa is the level of <text>, <aspectTerms>, <aspectCategories>\n","        if(taa.tag == 'text'):\n","            texts.append(taa.text)\n","            #print(sentence.tag, taa.tag, ':', taa.text)\n","        elif(taa.tag == 'aspectTerms'): \n","            hasTerm = True\n","            termInEachText = []\n","            for aspectTerms in taa:\n","                #print(sentence.tag, taa.tag, ':', aspectTerms.attrib) # attrib is dictionary\n","                term = aspectTerms.attrib['term']\n","                polarity = aspectTerms.attrib['polarity']\n","                termInEachText.append(term)\n","            if(polarity == 'positive'):\n","                term_positive[term] = term_positive.get(term, 0) + 1\n","            elif(polarity == 'negative'):\n","                term_negative[term] = term_negative.get(term, 0) + 1\n","            elif(polarity == 'neutral'):\n","                term_neutral[term] = term_neutral.get(term, 0) + 1\n","            elif(polarity == 'conflict'):\n","                term_conflict[term] = term_conflict.get(term, 0) + 1\n","        elif(taa.tag == 'aspectCategories'): \n","            hasCategory = True\n","            categoryInEachText = []\n","            for aspectCategories in taa:\n","                #print(sentence.tag, taa.tag, ':', aspectCategories.attrib)\n","                category = aspectCategories.attrib['category']\n","                polarity = aspectCategories.attrib['polarity']\n","                categoryInEachText.append(category)\n","            #categories.append(categoryInEachText)\n","            if(polarity == 'positive'):\n","                category_positive[category] = category_positive.get(category, 0) + 1\n","            elif(polarity == 'negative'):\n","                category_negative[category] = category_negative.get(category, 0) + 1\n","            elif(polarity == 'neutral'):\n","                category_neutral[category] = category_neutral.get(category, 0) + 1\n","            elif(polarity == 'conflict'):\n","                category_conflict[category] = category_conflict.get(category, 0) + 1\n","                \n","    if(hasTerm == True):\n","        terms.append(termInEachText)\n","    elif(hasTerm == False):\n","        terms.append([])\n","    \n","    if(hasCategory == True):\n","        categories.append(categoryInEachText)\n","    elif(hasCategory == False):\n","        categories.append([])"]},{"cell_type":"markdown","metadata":{"id":"dW8zhxjuCIE8"},"source":["## Print to view the result\n","\n","Currently we will use **texts** for preprocessing to make texts in IOB format. The remaining dictionary is for further use."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"1-XFqdaFCIE-","outputId":"cb11764c-dc60-4c63-d8c6-8edf4c41e854"},"outputs":[{"data":{"text/plain":["\"\\n# List: a list of 'text' or a sentence from XML.\\nprint(texts, '\\n')\\n\\n# Dictionary: {term: number of terms founded in all texts}\\nprint('term_positive\\n', term_positive , '\\n')\\nprint('term_negative\\n', term_negative , '\\n')\\nprint('term_neutral\\n', term_neutral , '\\n')\\nprint('term_conflict\\n', term_conflict , '\\n')\\n\\n# Dictionary: {category: number of category founded in all texts}\\nprint('category_positive\\n', category_positive , '\\n')\\nprint('category_negative\\n', category_negative , '\\n')\\nprint('category_neutral\\n', category_neutral , '\\n')\\nprint('category_conflict\\n', category_conflict , '\\n')\\n\""]},"execution_count":31,"metadata":{},"output_type":"execute_result"}],"source":["\"\"\"\n","# List: a list of 'text' or a sentence from XML.\n","print(texts, '\\n')\n","\n","# List: a list of 'term' in each text\n","print(terms, '\\n')\n","# List: a list of 'category' in each text\n","print(categories, '\\n')\n","\n","# Dictionary: {term: number of terms founded in all texts}\n","print('term_positive\\n', term_positive , '\\n')\n","print('term_negative\\n', term_negative , '\\n')\n","print('term_neutral\\n', term_neutral , '\\n')\n","print('term_conflict\\n', term_conflict , '\\n')\n","\n","# Dictionary: {category: number of category founded in all texts}\n","print('category_positive\\n', category_positive , '\\n')\n","print('category_negative\\n', category_negative , '\\n')\n","print('category_neutral\\n', category_neutral , '\\n')\n","print('category_conflict\\n', category_conflict , '\\n')\n","\"\"\""]},{"cell_type":"markdown","metadata":{"id":"_HK26HRiCIFB"},"source":["## Make a text to IOB format\n","\n","Nichy is working on Step 3 about Regex to extract IOB as close as 'terms'.\n","\n","Useful resource: https://www.nltk.org/book/ch07.html"]},{"cell_type":"code","source":["import nltk\n","nltk.download('punkt')\n","nltk.download('averaged_perceptron_tagger')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"z0Oq8DgoP8yT","executionInfo":{"status":"ok","timestamp":1648877234455,"user_tz":-480,"elapsed":1167,"user":{"displayName":"Nichamon Han-idhikul","userId":"15474088881495434738"}},"outputId":"daf91109-b154-4c56-f76d-7d860aadc8a5"},"execution_count":5,"outputs":[{"output_type":"stream","name":"stdout","text":["[nltk_data] Downloading package punkt to /root/nltk_data...\n","[nltk_data]   Unzipping tokenizers/punkt.zip.\n","[nltk_data] Downloading package averaged_perceptron_tagger to\n","[nltk_data]     /root/nltk_data...\n","[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n"]},{"output_type":"execute_result","data":{"text/plain":["True"]},"metadata":{},"execution_count":5}]},{"cell_type":"code","execution_count":18,"metadata":{"id":"m5_ZDV89CIFC","executionInfo":{"status":"ok","timestamp":1648877867060,"user_tz":-480,"elapsed":4139,"user":{"displayName":"Nichamon Han-idhikul","userId":"15474088881495434738"}}},"outputs":[],"source":["from nltk.tokenize import word_tokenize\n","from nltk.tag import pos_tag\n","from nltk.chunk.regexp import RegexpParser\n","from nltk.chunk import tree2conllstr\n","import nltk, string\n","\n","insideWords = []\n","x = []\n","\n","for t in texts:\n","    \"\"\" Step 1: Tokenization \"\"\"\n","    tokenizedText = word_tokenize(t)\n","    #tokenizedText = list(filter(lambda token: token not in string.punctuation, tokenizedText))\n","    \n","    \"\"\" Step 2: Part-of-speech tagging \"\"\"\n","    posText = pos_tag(tokenizedText)\n","\n","    \"\"\" Step 3: Entity detection \n","    This rule says that an NP chunk should be formed whenever the chunker finds an optional determiner (DT) followed by any number of adjectives (JJ) and then a noun (NN)\n","    \"\"\"\n","    #grammar = \"NP: {<DT>?<JJ>*<NN.*><IN>?<NN.*>*}\"\n","    grammar = \"NP: {<JJ|VBN>*<NN.*><IN>?<JJ|VBG|DT|IN>?<NN.*>*}\"\n","    chunkParser = RegexpParser(grammar)\n","    chunkedText = chunkParser.parse(posText)\n","    \n","    \"\"\" Step 4: Relation detection. This relates to IOB format. \"\"\"\n","    iobText = tree2conllstr(chunkedText)\n","    # x.append(iobText)\n","    x.append(getIOB(iobText))\n","    insideWord = getInsideOfIOB(iobText)\n","    insideWords.append(insideWord)\n","    \n","    "]},{"cell_type":"code","source":["def getIOB(iobText):\n","    iobWords = iobText.split('\\n')\n","    array = []\n","    for iob in iobWords:\n","      w = iob.split(\" \")[0]\n","      tag = iob.split(\" \")[2]\n","      array.append((w, tag))\n","    return array"],"metadata":{"id":"Acs5Iw6-wEfN","executionInfo":{"status":"ok","timestamp":1648877860691,"user_tz":-480,"elapsed":461,"user":{"displayName":"Nichamon Han-idhikul","userId":"15474088881495434738"}}},"execution_count":17,"outputs":[]},{"cell_type":"code","source":["x[:3]"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ezQ2N-J6yMue","executionInfo":{"status":"ok","timestamp":1648877951940,"user_tz":-480,"elapsed":20,"user":{"displayName":"Nichamon Han-idhikul","userId":"15474088881495434738"}},"outputId":"90da8f0c-7abd-4a79-be92-f9e99ebea5eb"},"execution_count":20,"outputs":[{"output_type":"execute_result","data":{"text/plain":["[[('But', 'O'),\n","  ('the', 'O'),\n","  ('staff', 'B-NP'),\n","  ('was', 'O'),\n","  ('so', 'O'),\n","  ('horrible', 'O'),\n","  ('to', 'O'),\n","  ('us', 'O'),\n","  ('.', 'O')],\n"," [('To', 'O'),\n","  ('be', 'O'),\n","  ('completely', 'O'),\n","  ('fair', 'O'),\n","  (',', 'O'),\n","  ('the', 'O'),\n","  ('only', 'B-NP'),\n","  ('redeeming', 'I-NP'),\n","  ('factor', 'I-NP'),\n","  ('was', 'O'),\n","  ('the', 'O'),\n","  ('food', 'B-NP'),\n","  (',', 'O'),\n","  ('which', 'O'),\n","  ('was', 'O'),\n","  ('above', 'O'),\n","  ('average', 'B-NP'),\n","  (',', 'O'),\n","  ('but', 'O'),\n","  ('could', 'O'),\n","  (\"n't\", 'O'),\n","  ('make', 'O'),\n","  ('up', 'O'),\n","  ('for', 'O'),\n","  ('all', 'O'),\n","  ('the', 'O'),\n","  ('other', 'B-NP'),\n","  ('deficiencies', 'I-NP'),\n","  ('of', 'I-NP'),\n","  ('Teodora', 'I-NP'),\n","  ('.', 'O')],\n"," [('The', 'O'),\n","  ('food', 'B-NP'),\n","  ('is', 'O'),\n","  ('uniformly', 'O'),\n","  ('exceptional', 'O'),\n","  (',', 'O'),\n","  ('with', 'O'),\n","  ('a', 'O'),\n","  ('very', 'O'),\n","  ('capable', 'B-NP'),\n","  ('kitchen', 'I-NP'),\n","  ('which', 'O'),\n","  ('will', 'O'),\n","  ('proudly', 'O'),\n","  ('whip', 'O'),\n","  ('up', 'O'),\n","  ('whatever', 'O'),\n","  ('you', 'O'),\n","  ('feel', 'O'),\n","  ('like', 'O'),\n","  ('eating', 'B-NP'),\n","  (',', 'O'),\n","  ('whether', 'O'),\n","  ('it', 'O'),\n","  (\"'s\", 'O'),\n","  ('on', 'O'),\n","  ('the', 'O'),\n","  ('menu', 'B-NP'),\n","  ('or', 'O'),\n","  ('not', 'O'),\n","  ('.', 'O')]]"]},"metadata":{},"execution_count":20}]},{"cell_type":"code","source":["import pandas as pd\n","df = pd.DataFrame(texts)\n","df['TextWithIOB'] = x\n"],"metadata":{"id":"K08FDXklvvGU"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/gdrive')\n","df.to_csv(\"/content/gdrive/MyDrive/Colab Notebooks/CS4248 NLP Project/iob.csv\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"b2zn8uYDzY_N","executionInfo":{"status":"ok","timestamp":1648878367130,"user_tz":-480,"elapsed":4549,"user":{"displayName":"Nichamon Han-idhikul","userId":"15474088881495434738"}},"outputId":"dd043fbd-d74c-4635-c07f-c987f217d806"},"execution_count":34,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/gdrive\n"]}]},{"cell_type":"code","source":["!ls"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"4XLw-NSrzgD_","executionInfo":{"status":"ok","timestamp":1648878371742,"user_tz":-480,"elapsed":22,"user":{"displayName":"Nichamon Han-idhikul","userId":"15474088881495434738"}},"outputId":"0dd762ec-cf06-4f0b-af51-1b382e85fd9c"},"execution_count":35,"outputs":[{"output_type":"stream","name":"stdout","text":["gdrive\tiob.csv  sample_data\n"]}]},{"cell_type":"markdown","source":["Once we get IOB format, we find nouns from each IOB.\n","### 1. Get single-word-nouns\n","\n","* Text:'Spice is great Thai food, love the inexpensive appetizers.'\n","* Given terms: ['Thai food', 'appetizers']\n","* IOB: ['Spice', 'Thai', 'food', 'love', 'appetizers']\n","\n","### 2. Get multiple-word nouns\n","\n","* Text: 'Spice is great Thai food, love the inexpensive appetizers.'\n","* Given terms: ['Thai food', 'appetizers']\n","* IOB: ['great Thai food', 'inexpensive appetizers']\n","      \n","You can see that the term is 'Thai food' but we get 'great Thai food'. This is because the code is set to include an adjective before a noun because some terms in other texts follow this pattern (e.g. term = 'green curry with vegetables'). If you fix 'great Thai food' to 'Thai food' then an issue will occurs with 'green curry with vegetatbles', it will become just 'curry with vegetatbles' which mismatches the given term. Therefore this is one of the issue to decide what pattern we have to follow."],"metadata":{"id":"oPkhqu3FAuDA"}},{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"id":"OmQzWC6fzl0e"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def getInsideOfIOB(iobText):\n","    iobWords = iobText.split('\\n')\n","    insideWords = []\n","    \n","    #1. Extract single-word nouns \n","    for i in range(len(iobWords)):\n","        word_and_iobs = iobWords[i].split(' ')\n","        word_and_iobs[0] = word_and_iobs[0].replace(\"'\", \"\")\n","        if(word_and_iobs[2] == 'B-NP' and word_and_iobs[1] not in ['DT', 'JJ']):\n","            insideWords.append(word_and_iobs[0])\n","        elif(word_and_iobs[2] == 'I-NP' and word_and_iobs[1] in ['NN', 'NNS', 'NNP', 'NNPS']):\n","            insideWords.append(word_and_iobs[0])\n","    \n","    # 2.  multiple-word nouns \n","    noun_string = ''\n","    for i in range(len(iobWords)):\n","        word_and_iobs = iobWords[i].split(' ')\n","        word_and_iobs[0] = word_and_iobs[0].replace(\"'\", \"\")\n","        if(word_and_iobs[2] == 'B-NP' and word_and_iobs[1] != 'DT'):\n","            noun_string = word_and_iobs[0]\n","        elif(word_and_iobs[2] == 'I-NP'):\n","            noun_string = noun_string + ' ' + word_and_iobs[0]\n","        elif(word_and_iobs[2] == 'O' and noun_string != '') :\n","            noun_string = removeSpaceBeforeNoun(noun_string)\n","            if ' ' in noun_string:\n","                insideWords.append(noun_string)\n","            noun_string = ''\n","        \n","        if(i == len(iobWords)-1 and noun_string != ''): # End of the text\n","            noun_string = removeSpaceBeforeNoun(noun_string)\n","            if ' ' in noun_string:\n","                insideWords.append(noun_string)\n","    return insideWords    \n","\n","def removeSpaceBeforeNoun(noun):\n","    if(noun[:1] == ' '):\n","        noun = noun[1:]\n","    return noun\n","\n"],"metadata":{"id":"eNNqqGP96hW_","executionInfo":{"status":"ok","timestamp":1648877227772,"user_tz":-480,"elapsed":644,"user":{"displayName":"Nichamon Han-idhikul","userId":"15474088881495434738"}}},"execution_count":4,"outputs":[]},{"cell_type":"markdown","source":["# Number of texts that keywords from IOB match to all terms"],"metadata":{"id":"HfeZRWB3EAgw"}},{"cell_type":"code","source":["import numpy as np\n","total_score = 0\n","for i in range(3044):\n","    score = checkTermInIOB(terms[i], insideWords[i])\n","    if(score == 1.0):\n","        total_score = total_score + 1\n","print('Number of texts that IOBs match terms:', total_score, 'out of', 3044)\n","print('Percentage:', np.round(total_score/3044, 4))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"8K5QZIXODenZ","executionInfo":{"status":"ok","timestamp":1647691317905,"user_tz":-480,"elapsed":358,"user":{"displayName":"Nichamon Han-idhikul","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"15474088881495434738"}},"outputId":"a1f75519-3f33-4bfb-caae-a3346c83fdda"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Number of texts that IOBs match terms: 2563 out of 3044\n","Percentage: 0.842\n"]}]},{"cell_type":"markdown","source":["# Text that some of its IOB is still cannot exactly match 'terms'"],"metadata":{"id":"Fvxa4pVm_55v"}},{"cell_type":"code","source":["def checkTermInIOB(terms, iobs):\n","    count = 0\n","    for t in terms:\n","        if t in iobs:\n","            count = count + 1\n","    #print(terms, iobs, count, len(terms))\n","    if(count == len(terms)):\n","        return 1.0\n","    elif(len(terms) == 0 ):\n","        return 1.0\n","    else:\n","        return 0.0\n","\n","for i in range(3044):\n","    score = checkTermInIOB(terms[i], insideWords[i])\n","    not_in_iob_words = []\n","    for t in terms[i]:\n","        if t not in insideWords[i]: \n","            not_in_iob_word = t\n","            not_in_iob_words.append(not_in_iob_word)\n","    #score = 0\n","    if(score != 1):\n","        print('ID:', i)\n","        #print('Terms not in IOB       :', not_in_iob_words)\n","        print('Terms      :', terms[i])\n","        print('IOB word   :', insideWords[i],'\\n')\n","\n"],"metadata":{"id":"6q917WvrAFJW"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import spacy\n","nlp = spacy.load(\"en_core_web_sm\")\n","doc = nlp(\"Apple is looking at buying U.K. startup for $1 billion\")"],"metadata":{"id":"eSLuYOR4v60w"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Tim's part"],"metadata":{"id":"mZ3Ip_Z5_8H8"}},{"cell_type":"markdown","source":["Place holder for Tim's pre-processing"],"metadata":{"id":"fdCOxpfPsWKP"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"JM4v-5-SCIFD"},"outputs":[],"source":["!pip install contractions\n","!pip install SpellChecker"]},{"cell_type":"code","source":["import xml.etree.ElementTree as ET\n","from enum import Enum\n","import pandas as pd\n","\n","restaurant = ET.parse('Restaurants_Train.xml')\n","sentences = restaurant.getroot()\n","class Cols(Enum):\n","    ID = 1\n","    Text = 2\n","    AspectTerms = 3\n","    PreProcessed = 4\n","data = {Cols.ID:[],Cols.Text:[],Cols.AspectTerms:[]}\n","\n","for sentence in sentences:\n","    term_list = []\n","    data[Cols.ID].append(sentence.attrib['id'])\n","    for taa in sentence: # taa is the level of <text>, <aspectTerms>, <aspectCategories>\n","        if(taa.tag == 'text'):\n","            data[Cols.Text].append(taa.text)\n","            #print(sentence.tag, taa.tag, ':', taa.text)\n","        elif(taa.tag == 'aspectTerms'): \n","           for aspectTerms in taa:\n","                #print(sentence.tag, taa.tag, ':', aspectTerms.attrib) # attrib is dictionary\n","                term = aspectTerms.attrib['term']\n","                polarity = aspectTerms.attrib['polarity']\n","                idx_from = aspectTerms.attrib['from']\n","                idx_to = aspectTerms.attrib['to']\n","                term_list.append((term,polarity,idx_from,idx_to))\n","    data[Cols.AspectTerms].append(term_list)\n","\n","df=pd.DataFrame.from_dict(data)\n","print(df)\n","\n","# TODO: need to check how does it work in google drive\n","df.to_csv(\"pre_processed.csv\")"],"metadata":{"id":"rJdqraW0soXL","colab":{"base_uri":"https://localhost:8080/","height":380},"executionInfo":{"status":"error","timestamp":1648820406749,"user_tz":-480,"elapsed":412,"user":{"displayName":"Nichamon Han-idhikul","userId":"15474088881495434738"}},"outputId":"e6044d67-ae4d-4157-c592-96473c59ecae"},"execution_count":null,"outputs":[{"output_type":"error","ename":"FileNotFoundError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-9-cb5fffa0a28e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpandas\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mrestaurant\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mET\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Restaurants_Train.xml'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0msentences\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrestaurant\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetroot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0mCols\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mEnum\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/lib/python3.7/xml/etree/ElementTree.py\u001b[0m in \u001b[0;36mparse\u001b[0;34m(source, parser)\u001b[0m\n\u001b[1;32m   1195\u001b[0m     \"\"\"\n\u001b[1;32m   1196\u001b[0m     \u001b[0mtree\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mElementTree\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1197\u001b[0;31m     \u001b[0mtree\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msource\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparser\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1198\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mtree\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1199\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/lib/python3.7/xml/etree/ElementTree.py\u001b[0m in \u001b[0;36mparse\u001b[0;34m(self, source, parser)\u001b[0m\n\u001b[1;32m    585\u001b[0m         \u001b[0mclose_source\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    586\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msource\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"read\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 587\u001b[0;31m             \u001b[0msource\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msource\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"rb\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    588\u001b[0m             \u001b[0mclose_source\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    589\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'Restaurants_Train.xml'"]}]},{"cell_type":"code","source":["import contractions\n","# all of the below act on a single text\n","\n","def fix_contractions(text):\n","    return contractions.fix(text)\n","\n","# return [(word, POS)......]\n","def get_pos(text)\n","    return nltk.posTag(nltk.word_tokenize(text))\n","\n","def basic_pre_process(text)\n","    return get_pos(fix_contractions(text))\n","\n","df[Cols.PreProcessed] = df[Cols.Text].map(basic_pre_process)\n","\n","\n","pos_replace = {\"CD\":\"<NUM>\",\"NNP\":\"<PNOUN>\",\"NNPS\":\"<PNOUN>\"}\n","\n","#3 required imput [(word, POS)......]\n","def replace(x):\n","    ls = []\n","    for word,tag in x:\n","        if tag in pos_replace:\n","            ls.append((pos_replace[tag],tag))\n","        # might not be useful here\n","        # elif any(char.isdigit() for char in word):\n","        #         ls.append((\"<NUM>\",\"CD\"))\n","        else:\n","            ls.append((word,tag))\n","    return ls\n","\n","def mispell_check(text):\n","    mispelled = [i[0] for i in text]\n","    return mispelled\n","\n","#TODO: investigating jamspell\n","#https://github.com/bakwc/JamSpell\n","#https://stackoverflow.com/questions/43984158/are-there-any-auto-correct-auto-complete-libraries-for-python\n","def mispell_fix(text):\n","    return text\n"],"metadata":{"id":"PQJ1utCh8Z-H"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Dependency Parsing\n"],"metadata":{"id":"hqFAFap2SjX5"}},{"cell_type":"code","source":["import spacy\n","from spacy import displacy\n","nlp = spacy.load(\"en_core_web_sm\")\n","text1 = \"The food is uniformly exceptional, with a very capable kitchen which will proudly whip up whatever you feel like eating, whether it's on the menu or not.\"\n","doc = nlp(text1)\n","for token in doc:\n","    print(token.text, token.tag_, token.head.text, token.dep_)\n","displacy.render(nlp(text1),jupyter=True)"],"metadata":{"id":"3Y57u6OuSiDx"},"execution_count":null,"outputs":[]}],"metadata":{"interpreter":{"hash":"8f9da245888fcfb163b7e6be5778ef91709c2888d1e99a2b1c9c6bced7960b6f"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.8"},"colab":{"name":"IOB Preprocessing.ipynb","provenance":[],"collapsed_sections":[]}},"nbformat":4,"nbformat_minor":0}